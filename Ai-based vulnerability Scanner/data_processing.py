import nltk
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib


# Save the fitted vectorizer for later use
# Assuming you have some training data available
# Reading the training data from a file
with open('training.txt', 'r', encoding='utf-8') as file:
    training_data = file.read().splitlines()

# Rest of your code, e.g., initializing and fitting the vectorizer

nltk.download('punkt')  # Download necessary data
vectorizer = TfidfVectorizer(stop_words=None)
vectorizer.fit(training_data)
joblib.dump(vectorizer, 'vectorizer.pkl')

nlp = spacy.load('en_core_web_sm')

#def tokenize(text):
 #   doc = nlp(text)
  #  return [token.text for token in doc]

def tokenize(text):
    return word_tokenize(text)

def preprocess_input_data(text_data):  # Use text_data as the parameter
    # Example: Tokenize and possibly vectorize the text
    tokens = tokenize(text_data)  # Convert string to tokens

    # Assuming vectorizer is defined and fit elsewhere
    vectorized_data = vectorizer.transform([text_data])
    print("Vectorized data shape:", vectorized_data.shape)  # Debugging line
    return vectorized_data.toarray()


