# data_processing/preprocess.py

import nltk
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib
import os

# Initialize NLTK resources
nltk.download('punkt')  # Download necessary NLTK data

# Paths
VECTORIZER_PATH = os.path.join('models', 'vectorizer.pkl')
TRAINING_DATA_PATH = os.path.join('training.txt')

# Load or Create Vectorizer
def load_vectorizer():
    if os.path.exists(VECTORIZER_PATH):
        vectorizer = joblib.load(VECTORIZER_PATH)
    else:
        # Initialize and fit vectorizer
        if not os.path.exists(TRAINING_DATA_PATH):
            raise FileNotFoundError(f"Training data file not found at {TRAINING_DATA_PATH}")
        with open(TRAINING_DATA_PATH, 'r', encoding='utf-8') as file:
            training_data = file.read().splitlines()
        if not training_data:
            raise ValueError("Training data is empty.")
        vectorizer = TfidfVectorizer(stop_words='english')
        vectorizer.fit(training_data)
        joblib.dump(vectorizer, VECTORIZER_PATH)
    return vectorizer

vectorizer = load_vectorizer()

def tokenize(text):
    if not isinstance(text, str):
        raise TypeError("Input text must be a string.")
    return word_tokenize(text)

def preprocess_input_data(text_data):
    """
    Preprocesses the input text data by tokenizing and vectorizing.

    Args:
        text_data (str): The input text.

    Returns:
        numpy.ndarray: The vectorized representation of the input text.
    """
    tokens = tokenize(text_data)  # Tokenize the text
    # Vectorize the text using the loaded TF-IDF vectorizer
    vectorized_data = vectorizer.transform([text_data])
    return vectorized_data.toarray()
