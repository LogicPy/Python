import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import logging
import time
import sys
from concurrent.futures import ThreadPoolExecutor
import threading
import queue
from requests.adapters import HTTPAdapter, Retry
import re
import dns.resolver

# ==== NVD API CONFIG ====
BASE_URL = "https://services.nvd.nist.gov/rest/json/cves/2.0/"
API_KEY = "8056f88d-6cbc-455d-a60e-5f9f0240b4d3"

session = requests.Session()
retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
session.mount("https://", HTTPAdapter(max_retries=retries))

# Configure logging
logging.basicConfig(
    filename='exploit_scanner.log',
    filemode='w',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# Thread-safe set for visited URLs
visited_urls = set()
visited_lock = threading.Lock()

# Define the maximum depth for crawling
MAX_DEPTH = 2

# Define the delay between requests per thread (in seconds)
REQUEST_DELAY = 1

# Thread-safe container for discovered vulnerabilities
discovered_issues = []
discovered_issues_lock = threading.Lock()

def record_issue(issue_type, message, url):
    """
    Safely record a discovered issue with type/category, a message/description, and the URL.
    """
    with discovered_issues_lock:
        discovered_issues.append({
            "type": issue_type,
            "message": message,
            "url": url
        })
    logging.warning(f"{issue_type}: {message} at {url}")
    print(f"[{issue_type}] {message} at {url}")

def is_allowed(url, user_agent='ExploitScannerBot'):
    """
    Checks the robots.txt file to see if the crawler is allowed to access the URL.
    """
    parsed_url = urlparse(url)
    robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
    try:
        response = requests.get(robots_url, timeout=5, headers={'User-Agent': user_agent})
        if response.status_code == 200:
            lines = response.text.splitlines()
            user_agent_directives = False
            allowed = True
            for line in lines:
                line = line.strip()
                if line.lower().startswith('user-agent'):
                    if user_agent.lower() in line.lower():
                        user_agent_directives = True
                    else:
                        user_agent_directives = False
                elif user_agent_directives:
                    if line.lower().startswith('disallow'):
                        path = line.split(':', 1)[1].strip()
                        if path == '' or urlparse(url).path.startswith(path):
                            allowed = False
            return allowed
        else:
            # If robots.txt is not found, assume allowed
            return True
    except requests.RequestException as e:
        logging.warning(f"Could not fetch robots.txt for {url}: {e}")
        return True

def process_url(q, exploits, max_depth):
    """
    Worker function to process URLs from the queue.
    """
    while True:
        try:
            current_url, current_depth = q.get(timeout=5)  # Wait for 5 seconds
        except queue.Empty:
            # If queue is empty for 5 seconds, exit the thread
            return

        with visited_lock:
            if current_url in visited_urls:
                q.task_done()
                continue
            visited_urls.add(current_url)

        if current_depth > max_depth:
            q.task_done()
            continue

        logging.info(f"Scanning URL: {current_url} (Depth: {current_depth})")
        print(f"Scanning URL: {current_url} (Depth: {current_depth})")
        try:
            headers = {'User-Agent': 'ExploitScannerBot'}
            response = session.get(current_url, headers=headers, timeout=10)
            response.raise_for_status()
        except requests.RequestException as e:
            logging.error(f"Failed to retrieve {current_url}: {e}")
            q.task_done()
            continue

        soup = BeautifulSoup(response.text, 'html.parser')
        page_text = soup.get_text()

        # 1) Search for known CVE strings in the page content
        found_exploits = [exploit for exploit in exploits if exploit in page_text]
        for exploit in found_exploits:
            record_issue("Exploit Found", f"CVE reference {exploit} discovered", current_url)

        # 2) Check common vulnerabilities (naive checks)
        check_common_vulnerabilities(current_url, response, soup)

        # 3) Find and enqueue all anchor tags' hrefs
        links = soup.find_all('a')
        for link in links:
            href = link.get('href')
            if href:
                # Handle relative URLs
                href = urljoin(current_url, href)
                parsed_href = urlparse(href)
                # Skip if not HTTP or HTTPS
                if parsed_href.scheme not in ['http', 'https']:
                    continue
                # Normalize URL (remove fragments)
                href = parsed_href._replace(fragment='').geturl()
                # Respect robots.txt
                if not is_allowed(href):
                    logging.info(f"Disallowed by robots.txt: {href}")
                    continue
                # Enqueue the new URL with incremented depth
                with visited_lock:
                    if href not in visited_urls:
                        q.put((href, current_depth + 1))

        # Politeness delay
        time.sleep(REQUEST_DELAY)
        q.task_done()

def check_common_vulnerabilities(current_url, response, soup):
    """
    Perform basic checks for common web security issues.
    """
    page_text = response.text
    parsed_url = urlparse(current_url)
    query_params = parse_qs(parsed_url.query)

    # 1. Reflected XSS (very naive)
    for param, values in query_params.items():
        for val in values:
            if val in page_text and len(val) > 2:
                record_issue(
                    "Reflected XSS?",
                    f"Parameter '{param}' with value '{val}' appears reflected",
                    current_url
                )

    # 2. Disclosure of secrets
    secret_patterns = [
        r'api[_-]?key\s*[=:]\s*[A-Za-z0-9_\-]+',
        r'secret[_-]?key\s*[=:]\s*[A-Za-z0-9_\-]+',
        r'BEGIN\s+PRIVATE\s+KEY',
        r'BEGIN\s+RSA\s+PRIVATE\s+KEY',
        r'ey[A-Za-z0-9]*\.[A-Za-z0-9_-]*\.[A-Za-z0-9_-]*',  # JWT-like tokens
    ]
    for pattern in secret_patterns:
        if re.search(pattern, page_text, re.IGNORECASE):
            record_issue(
                "Disclosure of Secrets",
                f"Possible secret found by pattern '{pattern}'",
                current_url
            )

    # 3. Insecure Direct Object Reference (IDOR)
    if re.search(r'/\d{1,6}(/|$|\?)', current_url):
        record_issue(
            "Potential IDOR",
            "Numeric ID pattern in path",
            current_url
        )
    for param, values in query_params.items():
        for val in values:
            if re.match(r'^\d{1,6}$', val):
                record_issue(
                    "Potential IDOR",
                    f"Numeric ID pattern in param '{param}': {val}",
                    current_url
                )

    # 4. Lack of Security Headers
    required_headers = [
        'Content-Security-Policy',
        'X-Frame-Options',
        'Strict-Transport-Security',
        'X-Content-Type-Options',
        'Cache-Control',
    ]
    missing_headers = [h for h in required_headers if h not in response.headers]
    if missing_headers:
        record_issue(
            "Lack of Security Headers",
            f"Missing: {missing_headers}",
            current_url
        )

    # 5. Failure to Invalidate Session - placeholder
    # (Requires advanced logic, so just logging a note)
    # (No direct call to record_issue here unless you want a placeholder.)

    # 6. Stored XSS (naive)
    xss_pattern = re.compile(r'(<script>|on\w+=)', re.IGNORECASE)
    if xss_pattern.search(page_text):
        record_issue(
            "Possible XSS",
            "Suspicious script/onEvent found",
            current_url
        )

    # 7. Content Spoofing (naive)
    for param, values in query_params.items():
        for val in values:
            # Check if param is re-injected into the page in a suspicious manner
            if f">{val}<" in page_text or f"'{val}'" in page_text:
                record_issue(
                    "Content Spoofing?",
                    f"Parameter value '{val}' rendered as content",
                    current_url
                )

    # 8. Open Redirect (naive)
    for param, values in query_params.items():
        for val in values:
            if val.startswith('http://') or val.startswith('https://'):
                # If the page is a redirect (3xx) and location header matches param
                if 300 <= response.status_code < 400 and 'Location' in response.headers:
                    location_header = response.headers['Location']
                    if location_header.startswith(val):
                        record_issue(
                            "Open Redirect",
                            f"Redirect param '{param}' = '{val}'",
                            current_url
                        )

    # 9. Directory Listing Enabled
    if re.search(r'Index of\s+/|Directory listing for', page_text, re.IGNORECASE):
        record_issue(
            "Directory Listing",
            "Possible directory listing found",
            current_url
        )

    # 10. Misconfigured DNS (naive)
    domain = parsed_url.netloc
    try:
        # Try to fetch MX records, no exception => DNS presumably okay
        dns.resolver.resolve(domain, 'MX')
    except (dns.resolver.NXDOMAIN, dns.exception.DNSException):
        record_issue(
            "Misconfigured DNS?",
            "Could not find MX record or DNS error",
            current_url
        )

def fetch_vulnerabilities(cve_id=None, keyword=None):
    """
    Fetch vulnerabilities from the NVD API by CVE ID or keyword.
    Returns a list of CVE IDs.
    """
    params = {"apiKey": API_KEY}
    
    # Add filters based on user input
    if cve_id:
        params["cveId"] = cve_id
    if keyword:
        params["keyword"] = keyword

    cve_list = []
    try:
        response = requests.get(BASE_URL, params=params)
        response.raise_for_status()
        data = response.json()

        for item in data.get("vulnerabilities", []):
            cve = item.get("cve")
            if cve:
                cve_id = cve.get('id')
                if cve_id:
                    cve_list.append(cve_id)
    except requests.RequestException as e:
        print(f"Error fetching vulnerabilities: {e}")

    return cve_list

def main():
    if len(sys.argv) < 2:
        print("Usage: python exploit_scanner.py <start_url> [--keyword KWD] [--cve CVEID]")
        sys.exit(1)

    start_url = sys.argv[1]
    parsed_start = urlparse(start_url)
    if not parsed_start.scheme.startswith('http'):
        print("Please provide a valid URL with http or https scheme.")
        sys.exit(1)

    # Parse optional CLI args
    keyword_arg = None
    cve_arg = None
    if "--keyword" in sys.argv:
        idx = sys.argv.index("--keyword")
        if idx + 1 < len(sys.argv):
            keyword_arg = sys.argv[idx + 1]
    if "--cve" in sys.argv:
        idx = sys.argv.index("--cve")
        if idx + 1 < len(sys.argv):
            cve_arg = sys.argv[idx + 1]

    # 1) Static CVE list (existing from your code)
    static_exploits = [
        # ... your existing CVEs ...
        "CVE-2025-0437",
        "CVE-2025-21132",
        "CVE-2025-21413",
        # etc. ...
    ]

    # 2) Dynamically fetched CVE IDs
    dynamic_cves = fetch_vulnerabilities(cve_id=cve_arg, keyword=keyword_arg)

    # Merge static + dynamic CVEs
    exploits = list(set(static_exploits + dynamic_cves))

    print("\nFetched dynamic CVEs from NVD (if any):", dynamic_cves, "\n")
    print("Combined CVE list for scanning:", exploits, "\n")

    # Initialize the queue and enqueue the start URL with depth 0
    q = queue.Queue()
    q.put((start_url, 0))

    # Number of worker threads
    NUM_WORKERS = 10  # Adjust as needed

    # Create a ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
        # Start worker threads
        for _ in range(NUM_WORKERS):
            executor.submit(process_url, q, exploits, MAX_DEPTH)

        # Wait until all tasks in the queue are processed
        q.join()

    logging.info("Scanning completed.")
    print("\nScanning completed.\n")

    # --- Summarize the issues discovered ---
    if discovered_issues:
        print("=== Summary of Discovered Issues ===")
        for i, issue in enumerate(discovered_issues, start=1):
            print(f"{i}. [{issue['type']}] {issue['message']} (URL: {issue['url']})")
    else:
        print("No vulnerabilities were recorded.")

if __name__ == "__main__":
    main()
