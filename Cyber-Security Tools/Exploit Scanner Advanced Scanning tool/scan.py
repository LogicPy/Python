import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import logging
import time
import sys
from concurrent.futures import ThreadPoolExecutor
import threading
import queue
from requests.adapters import HTTPAdapter, Retry

session = requests.Session()
retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
session.mount("https://", HTTPAdapter(max_retries=retries))

# Configure logging
logging.basicConfig(
    filename='exploit_scanner.log',
    filemode='w',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# Thread-safe set for visited URLs
visited_urls = set()
visited_lock = threading.Lock()

# Define the maximum depth for crawling
MAX_DEPTH = 2

# Define the delay between requests per thread (in seconds)
REQUEST_DELAY = 1

def is_allowed(url, user_agent='ExploitScannerBot'):
    """
    Checks the robots.txt file to see if the crawler is allowed to access the URL.
    """
    parsed_url = urlparse(url)
    robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
    try:
        response = requests.get(robots_url, timeout=5, headers={'User-Agent': user_agent})
        if response.status_code == 200:
            lines = response.text.splitlines()
            user_agent_directives = False
            allowed = True
            for line in lines:
                line = line.strip()
                if line.lower().startswith('user-agent'):
                    if user_agent.lower() in line.lower():
                        user_agent_directives = True
                    else:
                        user_agent_directives = False
                elif user_agent_directives:
                    if line.lower().startswith('disallow'):
                        path = line.split(':', 1)[1].strip()
                        if path == '' or urlparse(url).path.startswith(path):
                            allowed = False
            return allowed
        else:
            # If robots.txt is not found, assume allowed
            return True
    except requests.RequestException as e:
        logging.warning(f"Could not fetch robots.txt for {url}: {e}")
        return True

def process_url(q, exploits, max_depth):
    """
    Worker function to process URLs from the queue.
    """
    while True:
        try:
            current_url, current_depth = q.get(timeout=5)  # Wait for 5 seconds
        except queue.Empty:
            # If queue is empty for 5 seconds, exit the thread
            return

        with visited_lock:
            if current_url in visited_urls:
                q.task_done()
                continue
            visited_urls.add(current_url)

        if current_depth > max_depth:
            q.task_done()
            continue

        logging.info(f"Scanning URL: {current_url} (Depth: {current_depth})")
        print(f"Scanning URL: {current_url} (Depth: {current_depth})")
        try:
            headers = {'User-Agent': 'ExploitScannerBot'}
            response = requests.get(current_url, headers=headers, timeout=10)
            response.raise_for_status()
        except requests.RequestException as e:
            logging.error(f"Failed to retrieve {current_url}: {e}")
            q.task_done()
            continue

        soup = BeautifulSoup(response.text, 'html.parser')
        page_text = soup.get_text()

        # Search for exploits in the page content
        found_exploits = [exploit for exploit in exploits if exploit in page_text]
        for exploit in found_exploits:
            logging.info(f"Exploit found: {exploit} at {current_url}")
            print(f"Exploit found: {exploit} at {current_url}")

        # Find and enqueue all anchor tags' hrefs
        links = soup.find_all('a')
        for link in links:
            href = link.get('href')
            if href:
                # Handle relative URLs
                href = urljoin(current_url, href)
                parsed_href = urlparse(href)
                # Skip if not HTTP or HTTPS
                if parsed_href.scheme not in ['http', 'https']:
                    continue
                # Normalize URL (remove fragments)
                href = parsed_href._replace(fragment='').geturl()
                # Respect robots.txt
                if not is_allowed(href):
                    logging.info(f"Disallowed by robots.txt: {href}")
                    continue
                # Enqueue the new URL with incremented depth
                with visited_lock:
                    if href not in visited_urls:
                        q.put((href, current_depth + 1))

        # Politeness delay
        time.sleep(REQUEST_DELAY)
        q.task_done()

def main():
    # List of exploits to search for
    exploits = [
        "CVE-2024-53407",
        "CVE-2024-54535",
        "CVE-2025-0437",
        "CVE-2025-21132",
        "CVE-2025-21128",
        "CVE-2025-21129",
        "CVE-2025-21130",
        "CVE-2025-21131",
        "CVE-2025-21413",
        "CVE-2025-21417",
        "CVE-2025-21395",
        "CVE-2025-21409",
        "CVE-2025-21411",
        "CVE-2025-21374",
        "CVE-2025-21378",
        "CVE-2025-21382",
        "CVE-2025-21389",
        "CVE-2025-21393",
        "CVE-2025-21364",
        "CVE-2025-21365",
        "CVE-2025-21366",
        "CVE-2025-21370",
        "CVE-2025-21372",
        "CVE-2025-21361",
        "CVE-2025-21362",
        "CVE-2025-21363",
        "CVE-2025-21333",
        "CVE-2025-21334",
        "CVE-2025-21335",
        "CVE-2024-13179",
        "CVE-2024-55591",
        "CVE-2025-0282",
        "CVE-2024-3393",
        "CVE-2024-12686",
        "CVE-2024-12356",
        "CVE-2024-55956",
        "CVE-2024-49138",
        "CVE-2024-0012",
        "CVE-2024-43093",
        "CVE-2024-49039",
        "CVE-2024-43451",
        "CVE-2024-51378",
        "CVE-2024-51567",
        "CVE-2024-50623",
        "CVE-2024-20481",
        "CVE-2024-47575",
        "CVE-2024-41713",
        "CVE-2024-9537",
        "CVE-2024-9465",
        "CVE-2024-9463",
        "CVE-2019-16057",
        "CVE-2020-5902",
        "CVE-2020-25223",
        "CVE-2017-5753",
        "CVE-2019-1653",
        "CVE-2019-17558",
        "CVE-2020-2551",
        "CVE-2022-24706",
        "CVE-2022-47966",
        "CVE-2016-3081",
        "CVE-2021-22986",
        "CVE-2021-1497",
        "CVE-2021-1498",
        "CVE-2021-22502",
        "CVE-2022-35914",
        "CVE-2015-1635",
        "CVE-2022-37042",
        "CVE-2021-26855",
        "CVE-2020-7961",
        "CVE-2022-22963",
        "CVE-2015-0318",
        "CVE-2017-3881",
        "CVE-2017-9791",
        "CVE-2018-8120",
        "CVE-2005-1983"
    ]

    if len(sys.argv) != 2:
        print("Usage: python exploit_scanner.py <start_url>")
        sys.exit(1)

    start_url = sys.argv[1]
    parsed_start = urlparse(start_url)
    if not parsed_start.scheme.startswith('http'):
        print("Please provide a valid URL with http or https scheme.")
        sys.exit(1)

    # Initialize the queue and enqueue the start URL with depth 0
    q = queue.Queue()
    q.put((start_url, 0))

    # Number of worker threads
    NUM_WORKERS = 10  # Adjust as needed

    # Create a ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
        # Start worker threads
        for _ in range(NUM_WORKERS):
            executor.submit(process_url, q, exploits, MAX_DEPTH)

        # Wait until all tasks in the queue are processed
        q.join()

    logging.info("Scanning completed.")
    print("Scanning completed.")

if __name__ == "__main__":
    main()
