import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import logging
import time
import sys
from concurrent.futures import ThreadPoolExecutor
import threading
import queue
from requests.adapters import HTTPAdapter, Retry
import re
import random
import string
import dns.resolver

session = requests.Session()
retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
session.mount("https://", HTTPAdapter(max_retries=retries))

# Configure logging
logging.basicConfig(
    filename='exploit_scanner.log',
    filemode='w',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# Thread-safe set for visited URLs
visited_urls = set()
visited_lock = threading.Lock()

# Define the maximum depth for crawling
MAX_DEPTH = 2

# Define the delay between requests per thread (in seconds)
REQUEST_DELAY = 1

# List of CVE exploits to search for in page text
EXPLOITS = [
    "CVE-2024-53407",
    "CVE-2024-54535",
    "CVE-2025-0437",
    "CVE-2025-21132",
    "CVE-2025-21128",
    "CVE-2025-21129",
    "CVE-2025-21130",
    "CVE-2025-21131",
    "CVE-2025-21413",
    "CVE-2025-21417",
    "CVE-2025-21395",
    "CVE-2025-21409",
    "CVE-2025-21411",
    "CVE-2025-21374",
    "CVE-2025-21378",
    "CVE-2025-21382",
    "CVE-2025-21389",
    "CVE-2025-21393",
    "CVE-2025-21364",
    "CVE-2025-21365",
    "CVE-2025-21366",
    "CVE-2025-21370",
    "CVE-2025-21372",
    "CVE-2025-21361",
    "CVE-2025-21362",
    "CVE-2025-21363",
    "CVE-2025-21333",
    "CVE-2025-21334",
    "CVE-2025-21335",
    "CVE-2024-13179",
    "CVE-2024-55591",
    "CVE-2025-0282",
    "CVE-2024-3393",
    "CVE-2024-12686",
    "CVE-2024-12356",
    "CVE-2024-55956",
    "CVE-2024-49138",
    "CVE-2024-0012",
    "CVE-2024-43093",
    "CVE-2024-49039",
    "CVE-2024-43451",
    "CVE-2024-51378",
    "CVE-2024-51567",
    "CVE-2024-50623",
    "CVE-2024-20481",
    "CVE-2024-47575",
    "CVE-2024-41713",
    "CVE-2024-9537",
    "CVE-2024-9465",
    "CVE-2024-9463",
    "CVE-2019-16057",
    "CVE-2020-5902",
    "CVE-2020-25223",
    "CVE-2017-5753",
    "CVE-2019-1653",
    "CVE-2019-17558",
    "CVE-2020-2551",
    "CVE-2022-24706",
    "CVE-2022-47966",
    "CVE-2016-3081",
    "CVE-2021-22986",
    "CVE-2021-1497",
    "CVE-2021-1498",
    "CVE-2021-22502",
    "CVE-2022-35914",
    "CVE-2015-1635",
    "CVE-2022-37042",
    "CVE-2021-26855",
    "CVE-2020-7961",
    "CVE-2022-22963",
    "CVE-2015-0318",
    "CVE-2017-3881",
    "CVE-2017-9791",
    "CVE-2018-8120",
    "CVE-2005-1983"
]

def is_allowed(url, user_agent='ExploitScannerBot'):
    """
    Checks the robots.txt file to see if the crawler is allowed to access the URL.
    """
    parsed_url = urlparse(url)
    robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
    try:
        response = requests.get(robots_url, timeout=5, headers={'User-Agent': user_agent})
        if response.status_code == 200:
            lines = response.text.splitlines()
            user_agent_directives = False
            allowed = True
            for line in lines:
                line = line.strip()
                if line.lower().startswith('user-agent'):
                    if user_agent.lower() in line.lower():
                        user_agent_directives = True
                    else:
                        user_agent_directives = False
                elif user_agent_directives:
                    if line.lower().startswith('disallow'):
                        path = line.split(':', 1)[1].strip()
                        if path == '' or urlparse(url).path.startswith(path):
                            allowed = False
            return allowed
        else:
            # If robots.txt is not found or not 200, assume allowed
            return True
    except requests.RequestException as e:
        logging.warning(f"Could not fetch robots.txt for {url}: {e}")
        return True

def check_common_vulnerabilities(current_url, response, soup):
    """
    Perform basic checks for common web security issues:
      1. Reflected XSS (naive approach)
      2. Disclosure of secrets
      3. Insecure direct object reference (IDOR)
      4. Lack of security headers
      5. Failure to invalidate session (naive check – real check is complex)
      6. Stored XSS (naive check – real check is complex)
      7. Content spoofing (very naive approach)
      8. Open redirect
      9. Directory listing enabled
      10. Misconfigured DNS
    """

    # ----------------------------------
    # 1. Reflected XSS – NAIVE CHECK
    # ----------------------------------
    # We look for parameters in the URL, and see if any parameter values
    # are literally reflected in the response body. This is a simplistic approach.
    parsed_url = urlparse(current_url)
    query_params = parse_qs(parsed_url.query)
    page_text = response.text

    # If there's a param, check if param value is in the HTML
    for param, values in query_params.items():
        for val in values:
            # A naive check: if we see the raw param value in page, might indicate reflection
            if val in page_text and len(val) > 2:
                msg = f"[Reflected XSS?] Parameter '{param}' with value '{val}' appears reflected in {current_url}"
                logging.warning(msg)
                print(msg)

    # ----------------------------------
    # 2. Disclosure of secrets
    # ----------------------------------
    # Simple patterns for possible tokens, keys, etc.
    # (e.g. searching for "api_key", "secret_key", "BEGIN PRIVATE KEY", etc.)
    secret_patterns = [
        r'api[_-]?key\s*[=:]\s*[A-Za-z0-9_\-]+',
        r'secret[_-]?key\s*[=:]\s*[A-Za-z0-9_\-]+',
        r'BEGIN\s+PRIVATE\s+KEY',
        r'BEGIN\s+RSA\s+PRIVATE\s+KEY',
        r'ey[A-Za-z0-9]*\.[A-Za-z0-9_-]*\.[A-Za-z0-9_-]*',  # JWT-like tokens
    ]

    for pattern in secret_patterns:
        if re.search(pattern, page_text, re.IGNORECASE):
            msg = f"[Disclosure of secrets] Possible secret found by pattern '{pattern}' at {current_url}"
            logging.warning(msg)
            print(msg)

    # ----------------------------------
    # 3. Insecure Direct Object Reference (IDOR) – NAIVE CHECK
    # ----------------------------------
    # Very naive approach: if we see something that looks like a numeric ID in the URL,
    # we warn. Real IDOR detection would involve testing increments/decrements, etc.
    # Example pattern: /user/123, /product?id=456, etc.
    # We'll just log a message if we see typical numeric patterns in the path or query.
    if re.search(r'/\d{1,6}(/|$|\?)', current_url):
        msg = f"[Potential IDOR] Numeric ID pattern in path: {current_url}"
        logging.warning(msg)
        print(msg)
    for param, values in query_params.items():
        for val in values:
            if re.match(r'^\d{1,6}$', val):
                msg = f"[Potential IDOR] Numeric ID pattern in param '{param}': {val} at {current_url}"
                logging.warning(msg)
                print(msg)

    # ----------------------------------
    # 4. Lack of Security Headers
    # ----------------------------------
    # Check if certain recommended security headers are missing.
    # Example: Content-Security-Policy, X-Frame-Options, Strict-Transport-Security, X-Content-Type-Options
    required_headers = [
        'Content-Security-Policy',
        'X-Frame-Options',
        'Strict-Transport-Security',
        'X-Content-Type-Options',
        'Cache-Control',
    ]
    missing_headers = []
    for hdr in required_headers:
        if hdr not in response.headers:
            missing_headers.append(hdr)

    if missing_headers:
        msg = f"[Lack of Security Headers] Missing headers: {missing_headers} at {current_url}"
        logging.warning(msg)
        print(msg)

    # ----------------------------------
    # 5. Failure to Invalidate Session
    # ----------------------------------
    # True detection requires logging in, changing password, etc. This is a placeholder log only.
    # Real checks need advanced session and state tracking.
    logging.info("[Failure to Invalidate Session] Real detection requires advanced checks (placeholder).")

    # ----------------------------------
    # 6. Stored XSS – NAIVE CHECK
    # ----------------------------------
    # True detection usually involves input to the site, then retrieving it. 
    # We’ll do a naive check for script tags or on* events in the HTML.
    # This is not strictly "stored XSS," but might catch suspicious inline scripts.
    xss_pattern = re.compile(r'(<script>|on\w+=)', re.IGNORECASE)
    if xss_pattern.search(page_text):
        msg = f"[Possible XSS] Suspicious script/onEvent found in {current_url}"
        logging.warning(msg)
        print(msg)

    # ----------------------------------
    # 7. Content Spoofing – NAIVE CHECK
    # ----------------------------------
    # Real content spoofing is quite tricky to detect automatically.
    # As an extremely naive approach, look for parameters that contain strings
    # that appear as HTML in the response. This is somewhat overlapping with reflection (XSS).
    for param, values in query_params.items():
        for val in values:
            if f">{val}<" in page_text or f"'{val}'" in page_text:
                msg = f"[Content Spoofing?] Parameter value '{val}' rendered as content in {current_url}"
                logging.warning(msg)
                print(msg)

    # ----------------------------------
    # 8. Open Redirect – NAIVE CHECK
    # ----------------------------------
    # If a parameter contains "http://" or "https://" and the site returns a redirect
    # with that parameter or includes it in a link, it could be an open redirect issue.
    # We do a naive check for URL-like parameter values.
    for param, values in query_params.items():
        for val in values:
            if val.startswith('http://') or val.startswith('https://'):
                # Check if we have a 3xx redirect and the 'Location' header matches param value
                if 300 <= response.status_code < 400 and 'Location' in response.headers:
                    location_header = response.headers['Location']
                    if location_header.startswith(val):
                        msg = f"[Open Redirect] Redirect parameter '{param}' = '{val}' at {current_url}"
                        logging.warning(msg)
                        print(msg)

    # ----------------------------------
    # 9. Directory Listing Enabled – NAIVE CHECK
    # ----------------------------------
    # If the page looks like an autoindex, we detect it by searching for "Index of /..."
    if re.search(r'Index of\s+/|Directory listing for', response.text, re.IGNORECASE):
        msg = f"[Directory Listing] Possible directory listing found at {current_url}"
        logging.warning(msg)
        print(msg)

    # ----------------------------------
    # 10. Misconfigured DNS – NAIVE CHECK
    # ----------------------------------
    # A real check might look for missing MX, SPF, or DMARC records, etc.
    # Here we do a naive check for existence of MX records, for example.
    domain = parsed_url.netloc
    try:
        answers = dns.resolver.resolve(domain, 'MX')
        # If no exception: we have MX records. Not necessarily misconfigured.
        # Check for known anti-spam records? SPF check would require more parsing.
        pass
    except (dns.resolver.NXDOMAIN, dns.exception.DNSException):
        msg = f"[Misconfigured DNS?] Could not find MX record or DNS error for {domain}"
        logging.warning(msg)
        print(msg)

def process_url(q, exploits, max_depth):
    """
    Worker function to process URLs from the queue.
    """
    while True:
        try:
            current_url, current_depth = q.get(timeout=5)  # Wait for 5 seconds
        except queue.Empty:
            # If queue is empty for 5 seconds, exit the thread
            return

        with visited_lock:
            if current_url in visited_urls:
                q.task_done()
                continue
            visited_urls.add(current_url)

        if current_depth > max_depth:
            q.task_done()
            continue

        logging.info(f"Scanning URL: {current_url} (Depth: {current_depth})")
        print(f"Scanning URL: {current_url} (Depth: {current_depth})")
        try:
            headers = {'User-Agent': 'ExploitScannerBot'}
            response = session.get(current_url, headers=headers, timeout=10)
            response.raise_for_status()
        except requests.RequestException as e:
            logging.error(f"Failed to retrieve {current_url}: {e}")
            q.task_done()
            continue

        soup = BeautifulSoup(response.text, 'html.parser')
        page_text = soup.get_text()

        # 1) Search for known CVE strings in the page content
        found_exploits = [exploit for exploit in exploits if exploit in page_text]
        for exploit in found_exploits:
            logging.info(f"Exploit found: {exploit} at {current_url}")
            print(f"Exploit found: {exploit} at {current_url}")

        # 2) Check common web security issues
        check_common_vulnerabilities(current_url, response, soup)

        # 3) Find and enqueue all anchor tags' hrefs
        links = soup.find_all('a')
        for link in links:
            href = link.get('href')
            if href:
                # Handle relative URLs
                href = urljoin(current_url, href)
                parsed_href = urlparse(href)
                # Skip if not HTTP or HTTPS
                if parsed_href.scheme not in ['http', 'https']:
                    continue
                # Normalize URL (remove fragments)
                href = parsed_href._replace(fragment='').geturl()
                # Respect robots.txt
                if not is_allowed(href):
                    logging.info(f"Disallowed by robots.txt: {href}")
                    continue
                # Enqueue the new URL with incremented depth
                with visited_lock:
                    if href not in visited_urls:
                        q.put((href, current_depth + 1))

        # Politeness delay
        time.sleep(REQUEST_DELAY)
        q.task_done()

def main():
    if len(sys.argv) != 2:
        print("Usage: python exploit_scanner.py <start_url>")
        sys.exit(1)

    start_url = sys.argv[1]
    parsed_start = urlparse(start_url)
    if not parsed_start.scheme.startswith('http'):
        print("Please provide a valid URL with http or https scheme.")
        sys.exit(1)

    # Initialize the queue and enqueue the start URL with depth 0
    q = queue.Queue()
    q.put((start_url, 0))

    # Number of worker threads
    NUM_WORKERS = 10  # Adjust as needed

    # Create a ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
        # Start worker threads
        for _ in range(NUM_WORKERS):
            executor.submit(process_url, q, EXPLOITS, MAX_DEPTH)

        # Wait until all tasks in the queue are processed
        q.join()

    logging.info("Scanning completed.")
    print("Scanning completed.")

if __name__ == "__main__":
    main()
