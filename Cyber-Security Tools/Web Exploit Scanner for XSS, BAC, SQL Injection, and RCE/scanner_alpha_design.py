import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import threading
from queue import Queue, Empty
import time
import logging

# Configuration and Constants
HEADERS = {'User-Agent': 'Mozilla/5.0'}
REQUEST_TIMEOUT = 10
MAX_THREADS = 10
MAX_DEPTH = 3
DELAY_BETWEEN_REQUESTS = 1  # seconds

# Initialize logging
logging.basicConfig(filename='vulnerability_scanner.log', level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger()

# Define the types of vulnerabilities
VULN_TYPES = {
    '1': 'BAC',  # Broken Access Control
    '2': 'XSS',  # Cross-Site Scripting
    '3': 'SQLi',  # SQL Injection
    '4': 'Command Injection',
    '5': 'Full Scan',
    '6': 'Clickjacking'  # Newly added Clickjacking
}

# Initialize a thread-safe queue for URLs to scan
url_queue = Queue()

# Initialize a thread-safe set for visited URLs
visited_urls = set()
visited_lock = threading.Lock()

# Initialize a thread-safe set for logged vulnerabilities
logged_vulnerabilities = set()
logged_vulnerabilities_lock = threading.Lock()

# Initialize a thread-safe counter for duplicate skips (if applicable)
duplicate_skips = 0
duplicate_skips_lock = threading.Lock()

def is_valid_url(url):
    """
    Validates the URL format.
    """
    parsed = urlparse(url)
    return bool(parsed.scheme) and bool(parsed.netloc)

def get_domain(url):
    """
    Extracts the domain from a URL.
    """
    parsed = urlparse(url)
    return parsed.netloc

def normalize_url(base_url, link):
    """
    Normalizes and resolves relative URLs.
    """
    return requests.compat.urljoin(base_url, link)

def handle_form(url, form, session, selected_vulns):
    """
    Handles form elements for potential vulnerabilities.
    """
    # Implement form handling logic based on selected vulnerabilities
    pass  # Placeholder for actual implementation

def check_clickjacking(url):
    """
    Checks the given URL for Clickjacking vulnerabilities by analyzing HTTP headers.
    """
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        headers = response.headers

        x_frame_options = headers.get('X-Frame-Options', None)
        csp = headers.get('Content-Security-Policy', None)

        vulnerabilities = []

        # Check X-Frame-Options
        if not x_frame_options:
            vulnerabilities.append('Missing X-Frame-Options header.')
        elif x_frame_options.upper() not in ['DENY', 'SAMEORIGIN']:
            vulnerabilities.append(f'Insecure X-Frame-Options value: {x_frame_options}')

        # Check Content-Security-Policy for frame-ancestors
        if not csp:
            vulnerabilities.append('Missing Content-Security-Policy header.')
        else:
            # Parse CSP for frame-ancestors
            directives = [directive.strip() for directive in csp.split(';')]
            frame_ancestors = None
            for directive in directives:
                if directive.startswith('frame-ancestors'):
                    frame_ancestors = directive
                    break
            if not frame_ancestors:
                vulnerabilities.append('Missing frame-ancestors directive in Content-Security-Policy.')
            else:
                # Basic check: should contain 'self' or trusted domains
                # Customize the trusted domains as per your requirements
                trusted_domains = ['self']
                if not any(domain in frame_ancestors for domain in trusted_domains):
                    vulnerabilities.append(f'Potentially insecure frame-ancestors directive: {frame_ancestors}')

        if vulnerabilities:
            with logged_vulnerabilities_lock:
                vuln_identifier = (url, 'Clickjacking')
                if vuln_identifier not in logged_vulnerabilities:
                    logged_vulnerabilities.add(vuln_identifier)
                    logger.warning(f"[!] Clickjacking vulnerabilities found in {url}:")
                    for vuln in vulnerabilities:
                        logger.warning(f"    - {vuln}")
                        print(f"[!] {vuln} in {url}")
        else:
            logger.info(f"[+] No Clickjacking vulnerabilities detected in {url}.")
            print(f"[+] No Clickjacking vulnerabilities detected in {url}.")

    except requests.exceptions.RequestException as e:
        logger.error(f"[-] Error accessing {url} during Clickjacking scan: {e}")
        print(f"[-] Error accessing {url} during Clickjacking scan: {e}")

def scan_url(url, session, selected_vulns, root_domain, max_depth=MAX_DEPTH):
    """
    Scan a single URL for selected vulnerabilities.
    """
    try:
        response = session.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT, allow_redirects=False)
        if response.status_code == 200:
            logger.info(f"[+] Accessing: {url}")
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract and enqueue all links
            for link in soup.find_all('a', href=True):
                href = link['href']
                normalized_href = normalize_url(url, href)
                parsed_href = urlparse(normalized_href)
                href_domain = get_domain(normalized_href)

                # Check if the URL belongs to the root domain
                if href_domain == root_domain and is_valid_url(normalized_href):
                    with visited_lock:
                        # Check depth if max_depth is set
                        if max_depth is not None:
                            # Extract depth based on number of '/' in path
                            path_depth = len(parsed_href.path.strip('/').split('/'))
                            current_depth = len(urlparse(url).path.strip('/').split('/'))
                            if path_depth > current_depth + max_depth:
                                logger.info(f"Skipping {normalized_href} due to depth limit.")
                                continue

                        if normalized_href not in visited_urls:
                            url_queue.put(normalized_href)
                else:
                    logger.info(f"Skipping external link: {normalized_href}")

            # Extract and handle all forms
            for form in soup.find_all('form'):
                handle_form(url, form, session, selected_vulns)

            # Perform Clickjacking Check if selected
            if 'Clickjacking' in selected_vulns:
                check_clickjacking(url)

    except requests.exceptions.RequestException as e:
        logger.error(f"[-] Error accessing {url}: {e}")

def worker(session, selected_vulns, root_domain, max_depth, stop_event):
    """
    Worker thread to process URLs from the queue.
    """
    while not stop_event.is_set():
        try:
            url = url_queue.get(timeout=1)  # Wait for 1 second
        except Empty:
            continue  # Check the stop_event again

        if url is None:
            url_queue.task_done()
            break  # Exit the thread

        with visited_lock:
            if url in visited_urls:
                url_queue.task_done()
                continue
            visited_urls.add(url)

        logger.info(f"Crawling URL: {url}")
        scan_url(url, session, selected_vulns, root_domain, max_depth)
        url_queue.task_done()
        time.sleep(DELAY_BETWEEN_REQUESTS)

def listen_for_quit(stop_event):
    """
    Listen for a quit signal (e.g., user pressing 'q') to gracefully stop scanning.
    """
    while not stop_event.is_set():
        user_input = input()
        if user_input.lower() == 'q':
            logger.info("Quit signal received. Stopping the scanner...")
            stop_event.set()
            break

def generate_summary():
    """
    Generate a summary report of the scanning process.
    """
    with logged_vulnerabilities_lock:
        total_vulns = len(logged_vulnerabilities)
    with visited_lock:
        total_scanned = len(visited_urls)

    summary = (
        "===== Vulnerability Scan Summary =====\n"
        f"Total URLs Scanned: {total_scanned}\n"
        f"Clickjacking Vulnerabilities Found: {total_vulns}\n"
        "======================================\n"
    )

    with open('vulnerability_summary.txt', 'w') as f:
        f.write(summary)

    logger.info(summary)
    print(summary)

def main():
    """
    Main function to run the vulnerability scanner.
    """
    # Display Interactive Menu
    print("======================================")
    print("  Welcome to the Vulnerability Scanner ")
    print("======================================")
    print("Select the type of vulnerabilities to scan for:")
    for key, value in VULN_TYPES.items():
        print(f"{key}. {value}")
    print("0. Exit")
    print("Press 'q' at any time to quit prematurely.")

    choice = input("Enter your choice (e.g., 1,2,5): ").strip()

    if choice.lower() == 'q':
        print("Quitting the scanner. Goodbye!")
        return

    if choice == '0':
        print("Exiting the scanner. Goodbye!")
        return

    selected_vulns = []
    choices = choice.split(',')

    for c in choices:
        c = c.strip()
        if c in VULN_TYPES:
            if VULN_TYPES[c] == 'Full Scan':
                selected_vulns = ['BAC', 'XSS', 'SQLi', 'Command Injection', 'Clickjacking']
                break
            else:
                vuln_key = VULN_TYPES[c].split(' (')[0] if '(' in VULN_TYPES[c] else VULN_TYPES[c]
                selected_vulns.append(vuln_key)

    if not selected_vulns:
        print("No valid choices selected. Exiting.")
        return

    print(f"Selected Vulnerabilities: {', '.join(selected_vulns)}")
    root_url = input("Enter the root URL to scan (e.g., http://localhost:3000): ").strip()
    if not is_valid_url(root_url):
        print("Invalid URL format. Please ensure it starts with http:// or https://")
        return

    # Extract the root domain from the root URL
    root_domain = get_domain(root_url)
    logger.info(f"Root Domain: {root_domain}")

    # Initialize session with redirect handling
    session = requests.Session()
    session.headers.update(HEADERS)
    adapter = requests.adapters.HTTPAdapter(max_retries=3)
    session.mount('http://', adapter)
    session.mount('https://', adapter)

    # Enqueue the root URL with depth 0
    url_queue.put(root_url)

    # Initialize stop_event
    stop_event = threading.Event()

    # Start the quit listener thread
    quit_thread = threading.Thread(target=listen_for_quit, args=(stop_event,))
    quit_thread.daemon = True
    quit_thread.start()

    # Start worker threads
    threads = []
    for _ in range(MAX_THREADS):
        t = threading.Thread(target=worker, args=(session, selected_vulns, root_domain, MAX_DEPTH, stop_event))
        t.daemon = True
        t.start()
        threads.append(t)

    # Wait until all URLs are processed or stop_event is set
    try:
        while not url_queue.empty():
            if stop_event.is_set():
                break
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("KeyboardInterrupt detected. Stopping the scanner...")
        stop_event.set()

    # If stop_event is set, enqueue 'None' to stop workers
    if stop_event.is_set():
        for _ in range(MAX_THREADS):
            url_queue.put(None)

    # Wait for all URLs to be processed
    url_queue.join()

    # Wait for all threads to finish
    for t in threads:
        t.join()

    # Generate summary report
    generate_summary()
    print("Scanning completed. Check 'vulnerability_scanner.log' and 'vulnerability_summary.txt' for details.")

if __name__ == "__main__":
    main()
